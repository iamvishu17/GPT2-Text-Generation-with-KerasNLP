{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamvishu17/GPT2-Text-Generation-with-KerasNLP/blob/main/Copy_of_gpt2_text_generation_with_kerasnlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH7qFMECuaWA"
      },
      "source": [
        "# GPT2 Text Generation with KerasNLP\n",
        "\n",
        "**Author:** Chen Qian<br>\n",
        "**Date created:** 2023/04/17<br>\n",
        "**Last modified:** 2024/04/12<br>\n",
        "**Description:** Use KerasNLP GPT2 model and `samplers` to do text generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mw7MxtrquaWE"
      },
      "source": [
        "In this tutorial, you will learn to use [KerasNLP](https://keras.io/keras_nlp/) to load a\n",
        "pre-trained Large Language Model (LLM) - [GPT-2 model](https://openai.com/research/better-language-models)\n",
        "(originally invented by OpenAI), finetune it to a specific text style, and\n",
        "generate text based on users' input (also known as prompt). You will also learn\n",
        "how GPT2 adapts quickly to non-English languages, such as Chinese."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaAkKOIiuaWE"
      },
      "source": [
        "##  Before we begin\n",
        "\n",
        "Colab offers different kinds of runtimes. Make sure to go to **Runtime ->\n",
        "Change runtime type** and choose the GPU Hardware Accelerator runtime\n",
        "(which should have >12G host RAM and ~15G GPU RAM) since you will finetune the\n",
        "GPT-2 model. Running this tutorial on CPU runtime will take hours."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIgfRXl8uaWF"
      },
      "source": [
        "## Install KerasNLP, Choose Backend and Import Dependencies\n",
        "\n",
        "This examples uses [Keras 3](https://keras.io/keras_3/) to work in any of\n",
        "`\"tensorflow\"`, `\"jax\"` or `\"torch\"`. Support for Keras 3 is baked into\n",
        "KerasNLP, simply change the `\"KERAS_BACKEND\"` environment variable to select\n",
        "the backend of your choice. We select the JAX backend below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6TPKnbCuaWG",
        "outputId": "e334756f-c213-4c2b-c5c0-b8595c21400c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for keras-nlp (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.16.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/keras-team/keras-nlp.git -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAz9U3emuaWH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # or \"tensorflow\" or \"torch\"\n",
        "\n",
        "import keras_nlp\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "keras.mixed_precision.set_global_policy(\"mixed_float16\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JD7jeZpruaWI"
      },
      "source": [
        "## Introduction to Generative Large Language Models (LLMs)\n",
        "\n",
        "Large language models (LLMs) are a type of machine learning models that are\n",
        "trained on a large corpus of text data to generate outputs for various natural\n",
        "language processing (NLP) tasks, such as text generation, question answering,\n",
        "and machine translation.\n",
        "\n",
        "Generative LLMs are typically based on deep learning neural networks, such as\n",
        "the [Transformer architecture](https://arxiv.org/abs/1706.03762) invented by\n",
        "Google researchers in 2017, and are trained on massive amounts of text data,\n",
        "often involving billions of words. These models, such as Google [LaMDA](https://blog.google/technology/ai/lamda/)\n",
        "and [PaLM](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html),\n",
        "are trained with a large dataset from various data sources which allows them to\n",
        "generate output for many tasks. The core of Generative LLMs is predicting the\n",
        "next word in a sentence, often referred as **Causal LM Pretraining**. In this\n",
        "way LLMs can generate coherent text based on user prompts. For a more\n",
        "pedagogical discussion on language models, you can refer to the\n",
        "[Stanford CS324 LLM class](https://stanford-cs324.github.io/winter2022/lectures/introduction/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgVRO7ZZuaWI"
      },
      "source": [
        "## Introduction to KerasNLP\n",
        "\n",
        "Large Language Models are complex to build and expensive to train from scratch.\n",
        "Luckily there are pretrained LLMs available for use right away. [KerasNLP](https://keras.io/keras_nlp/)\n",
        "provides a large number of pre-trained checkpoints that allow you to experiment\n",
        "with SOTA models without needing to train them yourself.\n",
        "\n",
        "KerasNLP is a natural language processing library that supports users through\n",
        "their entire development cycle. KerasNLP offers both pretrained models and\n",
        "modularized building blocks, so developers could easily reuse pretrained models\n",
        "or stack their own LLM.\n",
        "\n",
        "In a nutshell, for generative LLM, KerasNLP offers:\n",
        "\n",
        "- Pretrained models with `generate()` method, e.g.,\n",
        "    `keras_nlp.models.GPT2CausalLM` and `keras_nlp.models.OPTCausalLM`.\n",
        "- Sampler class that implements generation algorithms such as Top-K, Beam and\n",
        "    contrastive search. These samplers can be used to generate text with\n",
        "    custom models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOSGBOluuaWJ"
      },
      "source": [
        "## Load a pre-trained GPT-2 model and generate some text\n",
        "\n",
        "KerasNLP provides a number of pre-trained models, such as [Google\n",
        "Bert](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html)\n",
        "and [GPT-2](https://openai.com/research/better-language-models). You can see\n",
        "the list of models available in the [KerasNLP repository](https://github.com/keras-team/keras-nlp/tree/master/keras_nlp/models).\n",
        "\n",
        "It's very easy to load the GPT-2 model as you can see below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TI4hKOpHuaWJ",
        "outputId": "d1839930-cc2f-4278-952e-b1b5d8f460b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gpt2/keras/gpt2_base_en/2/download/metadata.json...\n",
            "100%|██████████| 141/141 [00:00<00:00, 94.9kB/s]\n",
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gpt2/keras/gpt2_base_en/2/download/preprocessor.json...\n",
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gpt2/keras/gpt2_base_en/2/download/tokenizer.json...\n",
            "100%|██████████| 448/448 [00:00<00:00, 416kB/s]\n",
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gpt2/keras/gpt2_base_en/2/download/assets/tokenizer/vocabulary.json...\n",
            "100%|██████████| 0.99M/0.99M [00:00<00:00, 2.09MB/s]\n",
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gpt2/keras/gpt2_base_en/2/download/assets/tokenizer/merges.txt...\n",
            "100%|██████████| 446k/446k [00:00<00:00, 1.32MB/s]\n",
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gpt2/keras/gpt2_base_en/2/download/task.json...\n",
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gpt2/keras/gpt2_base_en/2/download/config.json...\n",
            "100%|██████████| 484/484 [00:00<00:00, 466kB/s]\n",
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gpt2/keras/gpt2_base_en/2/download/model.weights.h5...\n",
            "100%|██████████| 475M/475M [00:09<00:00, 49.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "# To speed up training and generation, we use preprocessor of length 128\n",
        "# instead of full length 1024.\n",
        "preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(\n",
        "    \"gpt2_base_en\",\n",
        "    sequence_length=128,\n",
        ")\n",
        "gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(\n",
        "    \"gpt2_base_en\", preprocessor=preprocessor\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_1kw0f3uaWK"
      },
      "source": [
        "Once the model is loaded, you can use it to generate some text right away. Run\n",
        "the cells below to give it a try. It's as simple as calling a single function\n",
        "*generate()*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlYFwnUZuaWK",
        "outputId": "ad8e3e36-f9f9-4cca-cdef-260e1122936b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GPT-2 output:\n",
            "My trip to Yosemite was the most enjoyable of my life. I was there for three days, and I spent my time exploring the park and enjoying the views of the Yosemite Valley. The view from the summit was breathtaking. The view was also beautiful in the fall, and the views from the summit were beautiful. The view was beautiful in the fall, and the views from the summit were beautiful.\n",
            "\n",
            "I had the chance to take some pictures of the views from the top of Yosemite, which was the best place to go. I had the chance to take some pictures of the views from the top of Yosemite, which was the best place to go. I had the chance to take some pictures of the views from the top of Yosemite, which was the best place to go. I had the chance to photograph Yosemite Valley, which is a beautiful place to visit. It's a beautiful area, but it was also the only time to see it. It was also one of my favorite places to\n",
            "TOTAL TIME ELAPSED: 8.70s\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "\n",
        "output = gpt2_lm.generate(\"My trip to Yosemite was\", max_length=200)\n",
        "print(\"\\nGPT-2 output:\")\n",
        "print(output)\n",
        "\n",
        "end = time.time()\n",
        "print(f\"TOTAL TIME ELAPSED: {end - start:.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fPoaRoAuaWK"
      },
      "source": [
        "Try another one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HN6yFn1suaWK",
        "outputId": "f04978f6-b4be-4ea8-e390-cc9c42512ec8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GPT-2 output:\n",
            "That Italian restaurant is now known as The Fizzie's.\n",
            "\n",
            "The restaurant, which has been closed for several years, will reopen this summer, the Italian outlet announced on its website.\n",
            "TOTAL TIME ELAPSED: 1.62s\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "\n",
        "output = gpt2_lm.generate(\"That Italian restaurant is\", max_length=200)\n",
        "print(\"\\nGPT-2 output:\")\n",
        "print(output)\n",
        "\n",
        "end = time.time()\n",
        "print(f\"TOTAL TIME ELAPSED: {end - start:.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnUokgaouaWL"
      },
      "source": [
        "Notice how much faster the second call is. This is because the computational\n",
        "graph is [XLA compiled](https://www.tensorflow.org/xla) in the 1st run and\n",
        "re-used in the 2nd behind the scenes.\n",
        "\n",
        "The quality of the generated text looks OK, but we can improve it via\n",
        "fine-tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iu6UlVyouaWL"
      },
      "source": [
        "## More on the GPT-2 model from KerasNLP\n",
        "\n",
        "Next up, we will actually fine-tune the model to update its parameters, but\n",
        "before we do, let's take a look at the full set of tools we have to for working\n",
        "with for GPT2.\n",
        "\n",
        "The code of GPT2 can be found\n",
        "[here](https://github.com/keras-team/keras-nlp/blob/master/keras_nlp/models/gpt2/).\n",
        "Conceptually the `GPT2CausalLM` can be hierarchically broken down into several\n",
        "modules in KerasNLP, all of which have a *from_preset()* function that loads a\n",
        "pretrained model:\n",
        "\n",
        "- `keras_nlp.models.GPT2Tokenizer`: The tokenizer used by GPT2 model, which is a\n",
        "    [byte-pair encoder](https://huggingface.co/course/chapter6/5?fw=pt).\n",
        "- `keras_nlp.models.GPT2CausalLMPreprocessor`: the preprocessor used by GPT2\n",
        "    causal LM training. It does the tokenization along with other preprocessing\n",
        "    works such as creating the label and appending the end token.\n",
        "- `keras_nlp.models.GPT2Backbone`: the GPT2 model, which is a stack of\n",
        "    `keras_nlp.layers.TransformerDecoder`. This is usually just referred as\n",
        "    `GPT2`.\n",
        "- `keras_nlp.models.GPT2CausalLM`: wraps `GPT2Backbone`, it multiplies the\n",
        "    output of `GPT2Backbone` by embedding matrix to generate logits over\n",
        "    vocab tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02sMfgJwuaWL"
      },
      "source": [
        "## Finetune on Reddit dataset\n",
        "\n",
        "Now you have the knowledge of the GPT-2 model from KerasNLP, you can take one\n",
        "step further to finetune the model so that it generates text in a specific\n",
        "style, short or long, strict or casual. In this tutorial, we will use reddit\n",
        "dataset for example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hN7-krrguaWL"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "race_ds = tfds.load(\"race\", split=\"train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqwrxJZ3uaWM"
      },
      "source": [
        "Let's take a look inside sample data from the reddit TensorFlow Dataset. There\n",
        "are two features:\n",
        "\n",
        "- **__document__**: text of the post.\n",
        "- **__title__**: the title."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGpYLJ-DuaWM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f0c07e6-8fd7-42f7-944d-6736e9d4e8f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b\"Answering the Community Needs of Our City\\nThe Silver City Council  recognizes that citizens have certain needs.  To better meet your needs, we have made several changes to community facilities  in 2004. This chart shows how we have tried to make your life better.\\nTransport Three stations for the suburbs have been added to the western train service.20 new buses for the southern line were purchased in January. 50 percent of city bus-stops have been upgraded . Buses to the eastern suburbs will run every15 minutes.\\nCommunication  Broadband cable  is now available  to all parts of the city. All of the new Government buildings are ' smart'-wired for better computer service!\\nMedical Facilities The new state-of-the-art Nightingale Hospital was opened in June. To overcome a shortage of trained medical staff  at Dover Hospital, 10 doctors have been employed from overseas.Some facilities at Station Street Hospital have been upgraded.\\nEducation  Textbooks will be free to all primary students in 2004 ! Rent for private schools has been reduced. Teachers report that the 'no hat - no play' rule has been successful.\\nProtection and Security  Extra police now patrol ( ) the tourist areas. 50 new police officers graduated in July and have taken up duties in the city area.\\nEntertainment / Recreation  The John Street basketball courts have been re-surfaced ! The new Central Community Building opened in May. 5,000 new fiction books were bought for the Silver City Library.\"\n",
            "[b'The public notice is from   _  .' b'The notice is mainly about   _  .'\n",
            " b'All the following are true EXCEPT that   _'\n",
            " b'Which of the following changes would tourists to Silver City be most happy with?']\n",
            "[[b'the community' b'the local government' b'The citizens'\n",
            "  b'a travel agency']\n",
            " [b'the work carried out by the people of Silver City'\n",
            "  b'the facilities available in Silver City'\n",
            "  b'some improvements in Silver City'\n",
            "  b'information for interested tourists']\n",
            " [b'both residents and tourists can enjoy more security now'\n",
            "  b'Station Street Hospital had out-dated facilities before 2004'\n",
            "  b'primary students had to pay for their textbooks in 2003'\n",
            "  b'Dover Hospital is still short of trained medical staff']\n",
            " [b'Travel books are provided in the new library.'\n",
            "  b'Traveling by train is more convenient in Silver City.'\n",
            "  b'Free medical treatment is available at Station Street Hospital.'\n",
            "  b'There are more police officers on duty now.']]\n"
          ]
        }
      ],
      "source": [
        "for items in race_ds:\n",
        "    print(items['article'].numpy())\n",
        "    print(items['questions'].numpy())\n",
        "    print(items['options'].numpy())\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM4xgeabuaWM"
      },
      "source": [
        "In our case, we are performing next word prediction in a language model, so we\n",
        "only need the 'document' feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFecYJYFuaWM"
      },
      "outputs": [],
      "source": [
        "train_ds = (\n",
        "    race_ds.map(lambda items: items['article'])\n",
        "    .batch(32)\n",
        "    .cache()\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ksyt2LmuaWM"
      },
      "source": [
        "Now you can finetune the model using the familiar *fit()* function. Note that\n",
        "`preprocessor` will be automatically called inside `fit` method since\n",
        "`GPT2CausalLM` is a `keras_nlp.models.Task` instance.\n",
        "\n",
        "This step takes quite a bit of GPU memory and a long time if we were to train\n",
        "it all the way to a fully trained state. Here we just use part of the dataset\n",
        "for demo purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GkdqIgbuaWM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a61385e-4830-4981-9b77-fdd3fa74dd3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m586/586\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m285s\u001b[0m 427ms/step - accuracy: 0.3506 - loss: 3.3777\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x78903cb36980>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "train_ds = train_ds.take(1000)\n",
        "num_epochs = 1\n",
        "\n",
        "# 基于线性衰减的动态学习率\n",
        "learning_rate = keras.optimizers.schedules.PolynomialDecay(\n",
        "    5e-5,\n",
        "    decay_steps=train_ds.cardinality() * num_epochs,\n",
        "    end_learning_rate=0.0,\n",
        ")\n",
        "\n",
        "# 损失函数\n",
        "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# 模型编译\n",
        "gpt2_lm.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate),\n",
        "    loss=loss,\n",
        "    weighted_metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "gpt2_lm.fit(train_ds, epochs=num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "\n",
        "output = gpt2_lm.generate(\"I like to play Games\", max_length=200)\n",
        "print(\"\\nGPT-2 output:\")\n",
        "print(output)\n",
        "\n",
        "end = time.time()\n",
        "print(f\"TOTAL TIME ELAPSED: {end - start:.2f}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ip_0Y1dSvU0E",
        "outputId": "58dafb31-b8e3-4ac2-8b5a-926f988cbb15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GPT-2 output:\n",
            "I like to play Games for fun and to enjoy the game.                                                                                 The Game is an adventure game. It is a game where you play with friends, play with other friends, play with friends\n",
            "TOTAL TIME ELAPSED: 6.71s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSpvVG9xuaWN"
      },
      "source": [
        "After fine-tuning is finished, you can again generate text using the same\n",
        "*generate()* function. This time, the text will be closer to Reddit writing\n",
        "style, and the generated length will be close to our preset length in the\n",
        "training set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4viBvlpuaWN"
      },
      "source": [
        "## Into the Sampling Method\n",
        "\n",
        "In KerasNLP, we offer a few sampling methods, e.g., contrastive search,\n",
        "Top-K and beam sampling. By default, our `GPT2CausalLM` uses Top-k search, but\n",
        "you can choose your own sampling method.\n",
        "\n",
        "Much like optimizer and activations, there are two ways to specify your custom\n",
        "sampler:\n",
        "\n",
        "- Use a string identifier, such as \"greedy\", you are using the default\n",
        "configuration via this way.\n",
        "- Pass a `keras_nlp.samplers.Sampler` instance, you can use custom configuration\n",
        "via this way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7FTgWHguaWN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5797c5e1-a4dc-4375-f87b-5b4120b0004b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GPT-2 output:\n",
            "I like basketball.I love it.I like to watch the game and I love to be involved in the game.\n",
            "My father and I have been friends for a long time.My mother has always been a big supporter of me.I think my dad was the first to say that basketball is a sport,not a religion.He said, \"We're going to be a team,not a team.We're going to be a basketball team.\"\n",
            "I remember his first game.He had a great shot and he had to go down for the game.He went down hard and he got the layup.I was so excited.My mom and I had a\n",
            "\n",
            "GPT-2 output:\n",
            "I like basketball. I like to play it. I like to watch it. I like to watch it. I like to watch it. I like to watch it. I like to watch it. I like to watch it. I like to watch it. I like to watch it. I like to watch it. I like to watch it. I like to watch it. I like to watch it. I like to watch it. I like to watch it. I like to watch it. I like to watch it. I like to watch it. I like to watch it. I like to watch it. I like to watch it. I like to watch it. I like to watch it. I like to watch it.\n"
          ]
        }
      ],
      "source": [
        "# Use a string identifier.\n",
        "gpt2_lm.compile(sampler=\"top_k\")\n",
        "output = gpt2_lm.generate(\"I like basketball\", max_length=200)\n",
        "print(\"\\nGPT-2 output:\")\n",
        "print(output)\n",
        "\n",
        "# Use a `Sampler` instance. `GreedySampler` tends to repeat itself,\n",
        "greedy_sampler = keras_nlp.samplers.GreedySampler()\n",
        "gpt2_lm.compile(sampler=greedy_sampler)\n",
        "\n",
        "output = gpt2_lm.generate(\"I like basketball\", max_length=200)\n",
        "print(\"\\nGPT-2 output:\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXfCHTi-uaWN"
      },
      "source": [
        "For more details on KerasNLP `Sampler` class, you can check the code\n",
        "[here](https://github.com/keras-team/keras-nlp/tree/master/keras_nlp/samplers)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoLu_z6AuaWN"
      },
      "source": [
        "## Finetune on Chinese Poem Dataset\n",
        "\n",
        "We can also finetune GPT2 on non-English datasets. For readers knowing Chinese,\n",
        "this part illustrates how to fine-tune GPT2 on Chinese poem dataset to teach our\n",
        "model to become a poet!\n",
        "\n",
        "Because GPT2 uses byte-pair encoder, and the original pretraining dataset\n",
        "contains some Chinese characters, we can use the original vocab to finetune on\n",
        "Chinese dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COVz2eYXuaWN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1989eac1-aa8e-4f38-bee7-069da52b42c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'chinese-poetry'...\n",
            "remote: Enumerating objects: 7315, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 7315 (delta 2), reused 6 (delta 2), pack-reused 7309\u001b[K\n",
            "Receiving objects: 100% (7315/7315), 236.97 MiB | 22.55 MiB/s, done.\n",
            "Resolving deltas: 100% (5000/5000), done.\n",
            "Updating files: 100% (2285/2285), done.\n"
          ]
        }
      ],
      "source": [
        "!# Load chinese poetry dataset.\n",
        "!git clone https://github.com/chinese-poetry/chinese-poetry.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnmHcVthuaWN"
      },
      "source": [
        "Load text from the json file. We only use《全唐诗》for demo purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHN1dhKAuaWO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "poem_collection = []\n",
        "for file in os.listdir(\"chinese-poetry/全唐诗\"):\n",
        "    if \".json\" not in file or \"poet\" not in file:\n",
        "        continue\n",
        "    full_filename = \"%s/%s\" % (\"chinese-poetry/全唐诗\", file)\n",
        "    with open(full_filename, \"r\") as f:\n",
        "        content = json.load(f)\n",
        "        poem_collection.extend(content)\n",
        "\n",
        "paragraphs = [\"\".join(data[\"paragraphs\"]) for data in poem_collection]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZgY4wIVuaWO"
      },
      "source": [
        "Let's take a look at sample data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIq2cqXsuaWO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11c72dfa-c4d0-4dd8-ce64-ca9a89771147"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "摇落秋風急，蕭條易水寒。悲歌誰激烈，宵雅自平寬。本道尊中國，何庸伐可汗。山居聞備塞，莫厭腐儒餐。\n"
          ]
        }
      ],
      "source": [
        "print(paragraphs[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YP2nyUibuaWO"
      },
      "source": [
        "Similar as Reddit example, we convert to TF dataset, and only use partial data\n",
        "to train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKc9aqwtuaWO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0871d5bd-7942-4e23-8bbc-31102c6b0ee6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 194ms/step - accuracy: 0.2421 - loss: 2.8193\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x788ffdf7beb0>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "train_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices(paragraphs)\n",
        "    .batch(16)\n",
        "    .cache()\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "# Running through the whole dataset takes long, only take `500` and run 1\n",
        "# epochs for demo purposes.\n",
        "train_ds = train_ds.take(500)\n",
        "num_epochs = 1\n",
        "\n",
        "learning_rate = keras.optimizers.schedules.PolynomialDecay(\n",
        "    5e-4,\n",
        "    decay_steps=train_ds.cardinality() * num_epochs,\n",
        "    end_learning_rate=0.0,\n",
        ")\n",
        "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "gpt2_lm.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate),\n",
        "    loss=loss,\n",
        "    weighted_metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "gpt2_lm.fit(train_ds, epochs=num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoG0kflvuaWP"
      },
      "source": [
        "Let's check the result!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5J76C-jJw5L9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8Bz0bpduaWP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b2ee90d-2cc5-48b2-9b9a-3d480d0650dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "昨夜雨疏风骤，白馬清馬深游。白日陰石秋樹，江倚深塵倚風。\n"
          ]
        }
      ],
      "source": [
        "output = gpt2_lm.generate(\"昨夜雨疏风骤\", max_length=200)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json # Import the json module\n",
        "\n",
        "with open('/content/game_of_thrones.json', 'r', encoding='utf-8') as f:\n",
        "    game_of_thrones_data = json.load(f) # Now you can use json.load()"
      ],
      "metadata": {
        "id": "0sVLt1n4vYIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a TensorFlow dataset from the JSON data\n",
        "texts = game_of_thrones_data[\"texts\"]\n",
        "game_of_thrones_ds = tf.data.Dataset.from_tensor_slices(texts)"
      ],
      "metadata": {
        "id": "L3pYHy4WvYdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the dataset for training\n",
        "train_ds = (\n",
        "    game_of_thrones_ds\n",
        "    .batch(32)\n",
        "    .cache()\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")"
      ],
      "metadata": {
        "id": "v-db-ik5vYtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a subset for training (optional, depending on your needs)\n",
        "train_ds = train_ds.take(1000)\n",
        "num_epochs = 1\n",
        "\n",
        "# Configure a dynamically decreasing learning rate based on linear decay\n",
        "learning_rate = keras.optimizers.schedules.PolynomialDecay(\n",
        "    initial_learning_rate=5e-5,\n",
        "    decay_steps=train_ds.cardinality() * num_epochs,\n",
        "    end_learning_rate=0.0,\n",
        ")"
      ],
      "metadata": {
        "id": "D9VDHZHIvZWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the loss function\n",
        "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Assuming gpt2_lm is your GPT-2 model\n",
        "# If not already defined, you need to load or create your GPT-2 model here\n",
        "\n",
        "# Model compilation\n",
        "gpt2_lm.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate),\n",
        "    loss=loss,\n",
        "    weighted_metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "gpt2_lm.fit(train_ds, epochs=num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBt7h0uuvZpN",
        "outputId": "df198a5c-c52b-4520-f1d5-33e87991b9de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 351ms/step - accuracy: 0.2995 - loss: 0.6084\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x788fd7dfd630>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference\n",
        "start = time.time()\n",
        "output = gpt2_lm.generate(\"The Mother of Dragons\", max_length=200)\n",
        "print(\"\\nGPT-2 output:\")\n",
        "print(output)\n",
        "end = time.time()\n",
        "print(f\"TOTAL TIME ELAPSED: {end - start:.2f}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiPt3h_bvmJc",
        "outputId": "37d713e2-5ac3-457e-d3bb-100ed982463b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GPT-2 output:\n",
            "The Mother of Dragons,  she shouted,  you have a son,  but you will have to wait for him, for the time is up.\n",
            "TOTAL TIME ELAPSED: 7.19s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwH7DEbIuaWT"
      },
      "source": [
        "Not bad 😀"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}